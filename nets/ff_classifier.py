# coding=utf-8
"""
Python module for softmax binary classifier neural network
"""
import torch
import torch.nn as nn


def init_weights(net):
    """
    initialize the weights of a network
    :param net:
    :return:
    """

    # init parameters
    def init_module(m):
        if type(m) == nn.Linear:
            nn.init.xavier_normal(m.weight.data)
            nn.init.xavier_uniform(m.bias.data)

    net.apply(init_module)

    return net


def build_ff_classifier_noraiki(input_size, hidden_1_size, hidden_2_size, hidden_3_size, num_labels=2):
    """
    Constructs a neural net binary classifer
    :param input_size:
    :param hidden_1_size:
    :param hidden_2_size:
    :param hidden_3_size:
    :param num_labels:
    :return:
    """
    net = nn.Sequential(
        nn.Linear(input_size, hidden_1_size),
        nn.BatchNorm1d(hidden_1_size),
        nn.LeakyReLU(),
#        nn.ReLU(),
        nn.Linear(hidden_1_size, hidden_2_size),
        nn.ReLU(),
        nn.Linear(hidden_2_size, hidden_3_size),
        nn.ReLU(),
        nn.Linear(hidden_3_size, num_labels),
        nn.LogSoftmax(dim=1))

    return net

def build_ff_classifier(input_size, hidden_1_size, hidden_2_size, hidden_3_size, num_labels=2):
    """
    Constructs a neural net binary classifer
    :param input_size:
    :param hidden_1_size:
    :param hidden_2_size:
    :param hidden_3_size:
    :param num_labels:
    :return:
    """
    net = nn.Sequential(
        nn.Linear(input_size, hidden_1_size),
        nn.ReLU(),
        nn.Linear(hidden_1_size, hidden_2_size),
        nn.ReLU(),
        nn.Linear(hidden_2_size, hidden_3_size),
        nn.ReLU(),
        nn.Linear(hidden_3_size, num_labels),
        nn.LogSoftmax(dim=1))

    return net


## njust
class Attention(nn.Module):
    def __init__(self, input_size, output_dim, attention_dim):
        super(Attention, self).__init__()
        self.input_atten = nn.Linear(input_size, attention_dim)
        #self.hidden_atten = nn.Linear(output_dim, attention_dim)

        self.fully_atten = nn.Linear(attention_dim, output_dim)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, input_x, output):
        i_atten = self.input_atten(input_x)
        #o_atten = self.hidden_atten(output)
        f_atten = self.fully_atten(self.relu(i_atten))

        alpha = self.softmax(f_atten)
        return alpha


class MEEnsemble(nn.Module):
    def __init__(self, input_size, hidden_1_size, hidden_2_size, hidden_3_size, stacking_hidden_size, attention_dim, num_labels=2,
                 K=2):
        super(MEEnsemble, self).__init__()
        self.K = K
        self.sub_models = nn.ModuleList([build_ff_classifier(input_size, hidden_1_size, hidden_2_size, hidden_3_size, num_labels) for
                           _ in range(self.K)])

        #self.sub_models = [build_ff_classifier(input_size, hidden_1_size, hidden_2_size, hidden_3_size, num_labels) for
        #                   _ in range(self.K)]

        self.attention = Attention(input_size, num_labels * self.K, attention_dim)

        self.stacking_net = nn.Sequential(
            nn.ReLU(),
            nn.Linear(self.K * num_labels, stacking_hidden_size),
            nn.ReLU(),
            nn.Linear(stacking_hidden_size, num_labels),
            nn.LogSoftmax(dim=1)
        )

    def get_inter_output(self, seq_model, x, layer_idx = 7):
            for i in range(layer_idx):
                x = seq_model[i](x)
            return x

    def forward(self, x):
        #if x.is_cuda:
        #    pred_features = torch.cat([self.get_inter_output(self.sub_models[k].cuda(), x) for k in range(self.K)],
        #                              dim=1)
        #    alpha = self.attention(x, pred_features)

        #    weighted_features = alpha * pred_features
            # import sys
            # print(self.get_inter_output(self.sub_models[0].cuda(), x))
            # sys.exit(1)
        #    return self.stacking_net.cuda()(weighted_features)
        #else:
        pred_features = torch.cat([self.get_inter_output(self.sub_models[k], x) for k in range(self.K)], dim=1)
        #print(pred_features[0])
        alpha = self.attention(x, pred_features)
        weighted_features = alpha * pred_features

        return self.stacking_net(weighted_features)
